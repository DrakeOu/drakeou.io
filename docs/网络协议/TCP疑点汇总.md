# TCP疑点汇总

> ref: [不为人知的网络编程（二）](http://www.52im.net/thread-1004-1-1.html)
>
> [不为人知的网络编程（一）](http://www.52im.net/thread-1003-1-1.html)



TCP的问题主要在这么几点：

1. 网络连接的可靠保证（三握四挥）
2. 数据传输的可靠保证（超时重传）
3. 对网络环境公平性的保证（拥塞控制）
4. 传输的流量控制（滑动窗口）



## 三次握手，四次挥手

贴下整个流程的状态机图

![105123g2lejy56ffwyeyef](..\static\TCP\105123g2lejy56ffwyeyef.jpg)



![105155l9egwdbkzd478cdy](..\static\TCP\105155l9egwdbkzd478cdy.jpg)

这个流程确实没有什么可以赘述的，主要是不要忘记每个状态的名字

- `Fin_Wait_1`：断开连接过程中，主动方发送`Fin`之后，等待对方`ACK`自己的请求的状态。这个状态是等待己方通信信道的断开完成。

- `Fin_Wait_2`：在主动方收到对方对自己`FIN`的`ACK`后，等待对方发起`FIN`时的状态。这个状态是己方已经断开了到被动方的单向信道后等待被动方发起断开`FIN`的过程。

- `TIME-WAIT`：在等到被动方发起的`FIN`之后将开始关闭连接`CLOSING`，发送`ACK`后进入这个状态。此状态是确保被动方的连接可以被可靠关闭，因为TCP的协议规范，不对`ACK`进行`ACK`，所以此时的`TIME-WAIT`等待方无法验证自己的`ACK`是否被收到，只有`FIN`的发送方可以通过`ACK`来确认自己的`FIN`是否被送达。

  这里听起来有点绕，但实际上确实如此，**超时重试的基础其实不是超时重发，而是凭借什么来确认流程正常无需超时**，所以主动方在接收到被动方发起的`FIN`请求后，也只能进行`2*MSL`时间的等待，来保证如果自己的`ACK`丢失了，还能接收到对方重试发来的`FIN`。（确保被动方连接的可靠断开）

  同样的，不论被动方的`FIN`包发送过程如何，在主动方进入`TIME_WAIT`状态后，两个MSL时间是网络空间中可能还存在`FIN`的最长时长。如果不等待，对方没有接收到`FIN`的`ACK`，进行重试，主动方会发现连接已经完全关闭，目前根本没有建立连接，会直接回一个`RST`重置连接，那么这将导致被动方报错``Connect rest by peer`，所以需要等待避免这种情况(**不等待对别人有什么后果**)；同样的，在这个时间内新建立的连接可能复用了之前连接的5个元素（源IP，目的IP，TCP，源端口，目的端口），导致新的连接被重试的`FIN`给错误关闭（**不等待对自己有什么后果**）。

  

### 各种疑点

#### TCP 连接的初始化序列号能否固定

（固定的序列号可能会使先后不同的连接相互影响）

**如果初始化序列号（缩写为ISN：Inital Sequence Number）可以固定，我们来看看会出现什么问题：**

- 假设ISN固定是1，Client和Server建立好一条TCP连接后，Client连续给Server发了10个包，这10个包不知怎么被链路上的路由器缓存了(路由器会毫无先兆地缓存或者丢弃任何的数据包)，这个时候碰巧Client挂掉了；
- 然后Client用同样的端口号重新连上Server，Client又连续给Server发了几个包，假设这个时候Client的序列号变成了5；
- 接着，之前被路由器缓存的10个数据包全部被路由到Server端了，Server给Client回复确认号10，这个时候，Client整个都不好了，这是什么情况？我的序列号才到5，你怎么给我的确认号是10了，整个都乱了。


[RFC793](https://tools.ietf.org/html/rfc793)中，建议ISN和一个假的时钟绑在一起，这个时钟会在每4微秒对ISN做加一操作，直到超过2^32，又从0开始，这需要4小时才会产生ISN的回绕问题，这几乎可以保证每个新连接的ISN不会和旧的连接的ISN产生冲突。这种递增方式的ISN，很容易让攻击者猜测到TCP连接的ISN，现在的实现大多是在一个基准值的基础上进行随机的。

#### TCP 的 Peer 两端同时断开连接

**由上面的“TCP协议状态机 ”图可以看出：**



- TCP的Peer端在收到对端的FIN包前发出了FIN包，那么该Peer的状态就变成了FIN_WAIT1；
- Peer在FIN_WAIT1状态下收到对端Peer对自己FIN包的ACK包的话，那么Peer状态就变成FIN_WAIT2；
- Peer在FIN_WAIT2下收到对端Peer的FIN包，在确认已经收到了对端Peer全部的Data数据包后，就响应一个ACK给对端Peer，然后自己进入TIME_WAIT状态。


但是如果Peer在FIN_WAIT1状态下首先收到对端Peer的FIN包的话，那么该Peer在确认已经收到了对端Peer全部的Data数据包后，就响应一个ACK给对端Peer，然后自己进入CLOSEING状态，Peer在CLOSEING状态下收到自己的FIN包的ACK包的话，那么就进入TIME WAIT 状态。于是，TCP的Peer两端同时发起FIN包进行断开连接，那么两端Peer可能出现完全一样的状态转移 FIN_WAIT1-->CLOSEING-->TIME_WAIT，也就会Client和Server最后同时进入TIME_WAIT状态。

#### TCP中的超时

所有超时重试的功能都会维护一个队列或者至少使用计时器并占用内存，而这段超时时间就会给攻击者耗尽服务器资源的时间和操作空间。

- SYN-REVD维护的半连接队列：服务端会对半连接进行指数退避的重试策略。对应SYN flood攻击，应对攻击策略
- TIME-WAIT：

#### TIME-WAIT的快速回收和重复使用

##### TIME_WAIT快速回收


linux下开启TIME_WAIT快速回收需要同时打开`tcp_tw_recycle`和`tcp_timestamps`(默认打开)两选项。Linux下快速回收的时间为3.5 * RTO（Retransmission Timeout），而一个RTO时间为200ms至120s。开启快速回收TIME_WAIT，可能会带来不进行TIME_WAIT中说的三点危险。

**为了避免这些危险，要求同时满足以下三种情况的新连接要被拒绝掉：**



- 1）来自同一个对端Peer的TCP包携带了时间戳；
- 2）之前同一台peer机器(仅仅识别IP地址，因为连接被快速释放了，没了端口信息)的某个TCP数据在MSL秒之内到过本Server；
- 3）Peer机器新连接的时间戳小于peer机器上次TCP到来时的时间戳，且差值大于重放窗口戳(TCP_PAWS_WINDOW)。初看起来正常的数据包同时满足下面3条几乎不可能， 因为机器的时间戳不可能倒流的，出现上述的3点均满足时，一定是老的重复数据包又回来了，丢弃老的SYN包是正常的。到此，似乎启用快速回收就能很大程度缓解TIME_WAIT带来的问题。但是，这里忽略了一个东西就是NAT。。。在一个NAT后面的所有Peer机器在Server看来都是一个机器，NAT后面的那么多Peer机器的系统时间戳很可能不一致，有些快，有些慢。这样，在Server关闭了与系统时间戳快的Client的连接后，在这个连接进入快速回收的时候，同一NAT后面的系统时间戳慢的Client向Server发起连接，这就很有可能同时满足上面的三种情况，造成该连接被Server拒绝掉。所以，在是否开启tcp_tw_recycle需要慎重考虑了。



##### TIME_WAIT重用


linux上比较完美的实现了TIME_WAIT重用问题。只要满足下面两点中的一点，一个TW状态的四元组(即一个socket连接)可以重新被新到来的SYN连接使用：



- 1）新连接SYN告知的初始序列号比TIME_WAIT老连接的末序列号大；
- 2）如果开启了tcp_timestamps，并且新到来的连接的时间戳比老连接的时间戳大。


要同时开启`tcp_tw_reuse`选项和`tcp_timestamps` 选项才可以开启TIME_WAIT重用，还有一个条件是：重用TIME_WAIT的条件是收到最后一个包后超过1s。细心的同学可能发现TIME_WAIT重用对Server端来说并没解决大量TIME_WAIT造成的资源消耗的问题，因为不管TIME_WAIT连接是否被重用，它依旧占用着系统资源。即便如此，TIME_WAIT重用还是有些用处的，它解决了整机范围拒绝接入的问题，虽然一般一个单独的Client是不可能在MSL内用同一个端口连接同一个服务的，但是如果Client做了bind端口那就是同个端口了。时间戳重用TIME_WAIT连接的机制的前提是IP地址唯一性，得出新请求发起自同一台机器，但是如果是NAT环境下就不能这样保证了，于是在NAT环境下，TIME_WAIT重用还是有风险的。

有些同学可能会混淆`tcp_tw_reuse`和`SO_REUSEADDR` 选项，认为是相关的一个东西，其实他们是两个完全不同的东西，可以说两个半毛钱关系都没。`tcp_tw_reuse`是内核选项，而`SO_REUSEADDR`用户态的选项，使用`SO_REUSEADDR`是告诉内核，如果端口忙，但TCP状态位于 TIME_WAIT ，可以重用端口。如果端口忙，而TCP状态位于其他状态，重用端口时依旧得到一个错误信息， 指明Address already in use”。如果你的服务程序停止后想立即重启，而新套接字依旧使用同一端口，此时 `SO_REUSEADDR` 选项非常有用。但是，使用这个选项就会有上述的三点危险，虽然发生的概率不大。

## 超时重传

重传分为两个讨论的问题

1. 重传的超时计算
2. 重传的机制

TCP的重传简单来说是基于时间驱动进行的，必须等待一个超时时间，无法对网络做出快速响应。而快速重传是一个以数据驱动为方式的算法，可以解决这个Timeout的问题，如果连续收到三个一样的ACK，那么就进行重传。

但TCP的ACK有这样的特性：

- 考虑到延迟确认，ACK会按当前收到的最大序列号进行ACK，表示小于这个值的包都全部收到。所以确认是批量确认的。
- 包的发送范围是由滑动窗口决定的（其实还有拥塞控制窗口的共同作用），如果后面的包不是连续的，有丢包，那么接收到的ACK只可能是对方接收到的连续包的最大值

所以重传的时候，可以明确知道丢包了，且知道丢的第一个包，但不能知道到底丢了多少包。

**于是，RFC2018提出了Selective Acknowledgment (SACK，选择确认)机制，SACK是TCP的扩展选项，包括：**

- 1）SACK允许选项（Kind=4,Length=2，选项只允许在有SYN标志的TCP包中）；
- 2）SACK信息选项（Kind=5,Length）。
  一个SACK的例子如下图，红框说明：接收端收到了0-5500，8000-8500，7000-7500，6000-6500的数据了，这样发送端就可以选择重传丢失的5500-6000，6500-7000，7500-8000的包：

![不为人知的网络编程(二)：浅析TCP协议中的疑难杂症(下篇)_2.jpg](http://www.52im.net/data/attachment/forum/201708/30/145735pwc9ccjvx96gvqnu.jpg)



SACK依靠接收端的接收情况反馈，解决了重传风暴问题，这样够了吗？接收端能不能反馈更多的信息呢？显然是可以的，于是，RFC2883对对SACK进行了扩展，提出了D-SACK，也就是利用第一块SACK数据中描述重复接收的不连续数据块的序列号参数，其他SACK数据则描述其他正常接收到的不连续数据。这样发送方利用第一块SACK，可以发现数据段被网络复制、错误重传、ACK丢失引起的重传、重传超时等异常的网络状况，使得发送端能更好调整自己的重传策略。

**D-SACK，有几个优点：**

- 1）发送端可以判断出，是发包丢失了，还是接收端的ACK丢失了。(发送方，重传了一个包，发现并没有D-SACK那个包，那么就是发送的数据包丢了；否则就是接收端的ACK丢了，或者是发送的包延迟到达了)；
- 2）发送端可以判断自己的RTO是不是有点小了，导致过早重传(如果收到比较多的D-SACK就该怀疑是RTO小了)；
- 3）发送端可以判断自己的数据包是不是被复制了。(如果明明没有重传该数据包，但是收到该数据包的D-SACK)；
- 4）发送端可以判断目前网络上是不是出现了有些包被delay了，也就是出现先发的包却后到了。

由上可以看出**重传机制的发展过程**

1. 简单的超时作为唯一因素
2. 使用**快速重传**引入数据因素解决单纯超时的被动局面
3. 使用`Selective Acknowladge`解决重传时的重传风暴问题（因为不知道需要重传多少，要么一条一条重传，要么丢包后的全部重传）
4. 使用`D-SACK`增强信息传递





## 流量控制

我们都知道TCP的流量控制是通过滑动窗口来实现的，滑动窗口是一个16bit的字段，最大表示65535个字节

> 另外在TCP的选项字段中还包含了一个TCP窗口扩大因子，option-kind为3，option-length为3个字节，option-data取值范围0-14。窗口扩大因子用来扩大TCP窗口，可把原来16bit的窗口，扩大为31bit。这个窗口是接收端告诉发送端自己还有多少缓冲区可以接收数据。于是发送端就可以根据这个接收端的处理能力来发送数据，而不会导致接收端处理不过来。也就是，发送端是根据接收端通知的窗口大小来调整自己的发送速率的，以达到端到端的流量控制。尽管流量控制看起来简单明了，就是发送端根据接收端的限制来控制自己的发送就好了。

关于滑动窗口有这么几个重要问题

### 发送方如何方便地知道哪些包可以发（滑动窗口）

TCP的滑动窗口就是解决这个问题，发送方和接受方将维护一个一样的发送窗口，那么发送方不需要额外的通信检查本地的窗口就可以知道可以发送的大小。

![150023u8ygkcocfga8pnac](..\static\TCP\150023u8ygkcocfga8pnac.jpg)

- [1]-已经发送并得到接收端ACK的;
- [2]-已经发送但还未收到接收端ACK的;
- [3]-未发送但允许发送的(接收方还有空间);
- [4]-未发送且不允许发送(接收方没空间了)。



![150038k843m5p8ry0fv55i](..\static\TCP\150038k843m5p8ry0fv55i.png)

目前发送窗口是32，Window大小是17.

![150038kfb8z0or5vze5a5l](..\static\TCP\150038kfb8z0or5vze5a5l.png)

收到了接收方的ACK36, 窗口左侧向右移动，同时Window值不变，右侧出现新的可以发送的区域。

### 出现0窗口时发送方只能被动等待么（窗口探测）

![150115j6d96mozon09nn9n](..\static\TCP\150115j6d96mozon09nn9n.jpg)

上面展示了接受方如何通过`Window`影响发送方

当出现0窗口时，发送方可以通过`Zero Window Probe`技术进行窗口探测，ZWP会在窗口为0后发送ZWP包给对方，来探测接受端目前的窗口大小，一般这个值会设置成3次，每次大约30-60秒。这里的重试机制同样涉及到一个针对TCP的攻击行为（零窗口攻击）

### 每次返回小窗口时都要直接发送么（粘包算法）

本质就是一个避免发送大量小包的问题。造成这个问题原因有二：1)接收端一直在通知一个小的窗口; 2)发送端本身问题，一直在发送小包。这个问题，TCP中有个术语叫Silly Window Syndrome(糊涂窗口综合症)。解决这个问题的思路有两，1)接收端不通知小窗口，2)发送端积累一下数据在发送。

思路1)是在接收端解决这个问题，David D Clark’s 方案，如果收到的数据导致window size小于某个值，就ACK一个0窗口，这就阻止发送端在发数据过来。等到接收端处理了一些数据后windows size 大于等于了MSS，或者buffer有一半为空，就可以通告一个非0窗口。思路2)是在发送端解决这个问题，有个著名的Nagle’s algorithm。

**Nagle 算法的规则：**

- [1]如果包长度达到 MSS ，则允许发送；
- [2]如果该包含有 FIN ，则允许发送；
- [3]设置了 TCP_NODELAY 选项，则允许发送；
- [4]设置 TCP_CORK 选项时，若所有发出去的小数据包（包长度小于 MSS ）均被确认，则允许发送；
- [5]上述条件都未满足，但发生了超时（一般为 200ms ），则立即发送。


规则[4]指出TCP连接上最多只能有一个未被确认的小数据包。从规则[4]可以看出Nagle算法并不禁止发送小的数据包(超时时间内)，而是避免发送大量小的数据包。由于Nagle算法是依赖ACK的，如果ACK很快的话，也会出现一直发小包的情况，造成网络利用率低。TCP_CORK选项则是禁止发送小的数据包(超时时间内)，设置该选项后，TCP会尽力把小数据包拼接成一个大的数据包（一个 MTU）再发送出去，当然也不会一直等，发生了超时（一般为 200ms ），也立即发送。Nagle 算法和CP_CORK 选项提高了网络的利用率，但是增加是延时。从规则[3]可以看出，设置TCP_NODELAY 选项，就是完全禁用Nagle 算法了。

## 拥塞控制

关于RTT和RTO的解释

- `RTT(Round Trip Time)`：一个连接的往返时间，即数据发送时刻到接收到确认的时刻的差值；
- `RTO(Retransmission Time Out)`：重传超时时间，即从数据发送时刻算起，超过这个时间便执行重传。



谈到拥塞控制，就要先谈谈拥塞的因素和本质。本质上，网络上拥塞的原因就是大家都想独享整个网络资源，对于TCP，端到端的流量控制必然会导致网络拥堵。这是因为TCP只看到对端的接收空间的大小，而无法知道链路上的容量，只要双方的处理能力很强，那么就可以以很大的速率发包，于是链路很快出现拥堵，进而引起大量的丢包，丢包又引发发送端的重传风暴，进一步加剧链路的拥塞。另外一个拥塞的因素是链路上的转发节点，例如路由器，再好的路由器只要接入网络，总是会拉低网络的总带宽，如果在路由器节点上出现处理瓶颈，那么就很容易出现拥塞。由于TCP看不到网络的状况，那么拥塞控制是必须的并且需要采用试探性的方式来控制拥塞，于是拥塞控制要完成两个任务：[1]公平性；[2]拥塞过后的恢复。

TCP发展到现在，拥塞控制方面的算法很多，其中Reno是目前应用最广泛且较为成熟的算法，下面着重介绍一下Reno算法(RFC5681)。介绍该算法前，首先介绍一个概念duplicate acknowledgment(冗余ACK、重复ACK)。

**一般情况下一个ACK被称为冗余ACK，要同时满足下面几个条件(对于SACK，那么根据SACK的一些信息来进一步判断)：**



- [1] 接收ACK的那端已经发出了一些还没被ACK的数据包
- [2] 该ACK没有捎带data
- [3] 该ACK的SYN和FIN位都是off的，也就是既不是SYN包的ACK也不是FIN包的ACK。
- [4] 该ACK的确认号等于接收ACK那端已经收到的ACK的最大确认号
- [5] 该ACK通知的窗口等接收该ACK的那端上一个收到的ACK的窗口。


**Reno算法包含4个部分：**

- [1]慢热启动算法 – Slow Start;
- [2]拥塞避免算法 – Congestion Avoidance;
- [3]快速重传 - Fast Retransimit;
- [4]快速恢复算法 – Fast Recovery。


TCP的拥塞控制主要原理依赖于一个拥塞窗口(cwnd)来控制，根据前面的讨论，我们知道有一个接收端通告的接收窗口(rwnd)用于流量控制；加上拥塞控制后，发送端真正的发送窗口=min(rwnd, cwnd)。关于cwnd的单位，在TCP中是以字节来做单位的，我们假设TCP每次传输都是按照MSS大小来发送数据，因此你可以认为cwnd按照数据包个数来做单位也可以理解，下面如果没有特别说明是字节，那么cwnd增加1也就是相当于字节数增加1个MSS大小。



### 1慢热启动算法 – Slow Start


慢启动体现了一个试探的过程，刚接入网络的时候先发包慢点，探测一下网络情况，然后在慢慢提速。不要一上来就拼命发包，这样很容易造成链路的拥堵，出现拥堵了在想到要降速来缓解拥堵这就有点成本高了，毕竟无数的先例告诫我们先污染后治理的成本是很高的。

**慢启动的算法如下(cwnd全称Congestion Window)：**



- 1）连接建好的开始先初始化cwnd = N，表明可以传N个MSS大小的数据。
- 2）每当收到一个ACK，++cwnd; 呈线性上升
- 3）每当过了一个RTT，cwnd = cwnd*2; 呈指数让升
- 4）还有一个慢启动门限ssthresh（slow start threshold），是一个上限，当cwnd >= ssthresh时，就会进入"拥塞避免算法 - Congestion Avoidance"。


根据RFC5681，如果MSS > 2190 bytes，则N = 2;如果MSS < 1095 bytes，则N = 4;如果2190 bytes >= MSS >= 1095 bytes，则N = 3;一篇Google的论文《An Argument for Increasing TCP’s Initial Congestion Window》建议把cwnd 初始化成了 10个MSS。Linux 3.0后采用了这篇论文的建议。



### 2拥塞避免算法 – Congestion Avoidance


慢启动的时候说过，cwnd是指数快速增长的，但是增长是有个门限ssthresh(一般来说大多数的实现ssthresh的值是65535字节)的，到达门限后进入拥塞避免阶段。

**在进入拥塞避免阶段后，cwnd值变化算法如下：**



- 1）每收到一个ACK，调整cwnd 为 (cwnd + 1/cwnd) * MSS个字节；
- 2）每经过一个RTT的时长，cwnd增加1个MSS大小。


TCP是看不到网络的整体状况的，那么TCP认为网络拥塞的主要依据是它重传了报文段。

**前面我们说过TCP的重传分两种情况：**



- 1）出现RTO超时，重传数据包。这种情况下，TCP就认为出现拥塞的可能性就很大，于是它反应非常'强烈'：
    \- [1] 调整门限ssthresh的值为当前cwnd值的1/2；
    \- [2] reset自己的cwnd值为1；
    \- [3] 然后重新进入慢启动过程。
- 2）在RTO超时前，收到3个duplicate ACK进行重传数据包。这种情况下，收到3个冗余ACK后说明确实有中间的分段丢失，然而后面的分段确实到达了接收端，因为这样才会发送冗余ACK，这一般是路由器故障或者轻度拥塞或者其它不太严重的原因引起的，因此此时拥塞窗口缩小的幅度就不能太大，此时进入快速重传。



### 3快速重传 - Fast Retransimit

**快速重传做的事情有：**

- 1） 调整门限ssthresh的值为当前cwnd值的1/2；
- 2） 将cwnd值设置为新的ssthresh的值；
- 3） 重新进入拥塞避免阶段。


在快速重传的时候，一般网络只是轻微拥堵，在进入拥塞避免后，cwnd恢复的比较慢。针对这个，“快速恢复”算法被添加进来，当收到3个冗余ACK时，TCP最后的[3]步骤进入的不是拥塞避免阶段，而是快速恢复阶段。



### 4快速恢复算法 – Fast Recovery


快速恢复的思想是“数据包守恒”原则，即带宽不变的情况下，在网络同一时刻能容纳数据包数量是恒定的。当“老”数据包离开了网络后，就能向网络中发送一个“新”的数据包。既然已经收到了3个冗余ACK，说明有三个数据分段已经到达了接收端，既然三个分段已经离开了网络，那么就是说可以在发送3个分段了。于是只要发送方收到一个冗余的ACK，于是cwnd加1个MSS。

**快速恢复步骤如下(在进入快速恢复前，cwnd 和 sshthresh已被更新为：sshthresh = cwnd /2，cwnd = sshthresh)：**



- 1）把cwnd设置为ssthresh的值加3，重传Duplicated ACKs指定的数据包
- 2）如果再收到 duplicated Acks，那么cwnd = cwnd +1
- 3）如果收到新的ACK，而非duplicated Ack，那么将cwnd重新设置为【3】中1）的sshthresh的值。然后进入拥塞避免状态。


细心的同学可能会发现快速恢复有个比较明显的缺陷就是：它依赖于3个冗余ACK，并假定很多情况下，3个冗余的ACK只代表丢失一个包。但是3个冗余ACK也很有可能是丢失了很多个包，快速恢复只是重传了一个包，然后其他丢失的包就只能等待到RTO超时了。超时会导致ssthresh减半，并且退出了Fast Recovery阶段，多个超时会导致TCP传输速率呈级数下降。出现这个问题的主要原因是过早退出了Fast Recovery阶段。为解决这个问题，提出了New Reno算法，该算法是在没有SACK的支持下改进Fast Recovery算法(SACK改变TCP的确认机制，把乱序等信息会全部告诉对方，SACK本身携带的信息就可以使得发送方有足够的信息来知道需要重传哪些包，而不需要重传哪些包)。

**具体改进如下：**



- 1）发送端收到3个冗余ACK后，重传冗余ACK指示可能丢失的那个包segment1，如果segment1的ACK通告接收端已经收到发送端的全部已经发出的数据的话，那么就是只丢失一个包，如果没有，那么就是有多个包丢失了；
- 2）发送端根据segment1的ACK判断出有多个包丢失，那么发送端继续重传窗口内未被ACK的第一个包，直到sliding window内发出去的包全被ACK了，才真正退出Fast Recovery阶段。


我们可以看到，拥塞控制在拥塞避免阶段，cwnd是加性增加的，在判断出现拥塞的时候采取的是指数递减。为什么要这样做呢？这是出于公平性的原则，拥塞窗口的增加受惠的只是自己，而拥塞窗口减少受益的是大家。这种指数递减的方式实现了公平性，一旦出现丢包，那么立即减半退避，可以给其他新建的连接腾出足够的带宽空间，从而保证整个的公平性。

至此，TCP的疑难杂症基本介绍完毕了，总的来说TCP是一个有连接的、可靠的、带流量控制和拥塞控制的端到端的协议。TCP的发送端能发多少数据，由发送端的发送窗口决定(当然发送窗口又被接收端的接收窗口、发送端的拥塞窗口限制)的，那么一个TCP连接的传输稳定状态应该体现在发送端的发送窗口的稳定状态上，这样的话，TCP的发送窗口有哪些稳定状态呢？

**TCP的发送窗口稳定状态主要有上面三种稳定状态：**

- 【1】接收端拥有大窗口的经典锯齿状：
  大多数情况下都是处于这样的稳定状态，这是因为，一般情况下机器的处理速度就是比较快，这样TCP的接收端都是拥有较大的窗口，这时发送端的发送窗口就完全由其拥塞窗口cwnd决定了；网络上拥有成千上万的TCP连接，它们在相互争用网络带宽，TCP的流量控制使得它想要独享整个网络，而拥塞控制又限制其必要时做出牺牲来体现公平性。于是在传输稳定的时候TCP发送端呈现出下面过程的反复：
  \- [1]用慢启动或者拥塞避免方式不断增加其拥塞窗口，直到丢包的发生；
  \- [2]然后将发送窗口将下降到1或者下降一半，进入慢启动或者拥塞避免阶段(要看是由于超时丢包还是由于冗余ACK丢包)；
  过程如下图：
  ![150831wria6pkkyzzjp6jj](..\static\TCP\150831wria6pkkyzzjp6jj.jpg)
- 【2】接收端拥有小窗口的直线状态：这种情况下是接收端非常慢速，接收窗口一直很小，这样发送窗口就完全有接收窗口决定了。由于发送窗口小，发送数据少，网络就不会出现拥塞了，于是发送窗口就一直稳定的等于那个较小的接收窗口，呈直线状态。
- 【3】两个直连网络端点间的满载状态下的直线状态：这种情况下，Peer两端直连，并且只有位于一个TCP连接，那么这个连接将独享网络带宽，这里不存在拥塞问题，在他们处理能力足够的情况下，TCP的流量控制使得他们能够跑慢整个网络带宽。